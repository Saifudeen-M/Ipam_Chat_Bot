from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from snowflake.snowpark import Session
from tempfile import gettempdir
import pandas as pd
import logging
import os
import time

# Load env vars
load_dotenv()
logging.basicConfig(level=logging.INFO)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    logging.info(f"{request.method} {request.url} completed in {duration:.2f}s with status {response.status_code}")
    return response

def create_snowflake_session():
    return Session.builder.configs({
        "account": os.getenv("SNOWFLAKE_ACCOUNT"),
        "user": os.getenv("SNOWFLAKE_USER"),
        "password": os.getenv("SNOWFLAKE_PASSWORD"),
        "warehouse": os.getenv("SNOWFLAKE_WAREHOUSE"),
        "database": os.getenv("SNOWFLAKE_DATABASE"),
        "schema": os.getenv("SNOWFLAKE_SCHEMA", "PUBLIC")
    }).create()

session = create_snowflake_session()

class TextQuery(BaseModel):
    question: str

@app.post("/chat")
def chat(query: TextQuery):
    if not query.question:
        raise HTTPException(status_code=400, detail="Question is required.")

    try:
        # Step 1: Call Cortex Analyst to convert NQL → SQL
        cortex_result = session.sql(f"""
            CALL SNOWFLAKE.CORTEX.ANALYZE(
                '{query.question}',
                'IPAM_TEMP_DB.PUBLIC.IPAM_TEMP_CA'
            )
        """).collect()

        if not cortex_result:
            return {"response": "⚠️ Cortex did not return any SQL."}

        generated_sql = cortex_result[0][0]
        print("Extracted SQL:", generated_sql)

        # Step 2: Run the SQL
        rows = session.sql(generated_sql).collect()
        if not rows:
            return {"response": "⚠️ No matching data found."}

        df = pd.DataFrame([row.as_dict() for row in rows])

        # Step 3: Export to CSV if needed
        if any(word in query.question.lower() for word in ["csv", "excel", "xlsx", "export"]):
            path = os.path.join(gettempdir(), "ip_data.csv")
            df.to_csv(path, index=False)
            return FileResponse(path, media_type="text/csv", filename="ip_data.csv")

        return {"response": df.to_dict(orient="records")}

    except Exception as e:
        logging.error(f"❌ Error running Cortex: {str(e)}")
        return {"response": f"❌ Error executing semantic query: {str(e)}"}
