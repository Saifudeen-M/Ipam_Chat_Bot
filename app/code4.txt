from io import BytesIO
from tempfile import gettempdir
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from snowflake.snowpark import Session
import logging
import time
import os
import re
import pandas as pd
from openai import OpenAI

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO)

# OpenAI client setup
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL_NAME = "gpt-4o-mini"

# FastAPI App
app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware for request logging
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    logging.info(f"{request.method} {request.url} completed in {duration:.2f}s with status {response.status_code}")
    return response

# Snowflake Session Setup
def create_snowflake_session():
    return Session.builder.configs({
        "account": os.getenv("SNOWFLAKE_ACCOUNT"),
        "user": os.getenv("SNOWFLAKE_USER"),
        "password": os.getenv("SNOWFLAKE_PASSWORD"),
        "warehouse": os.getenv("SNOWFLAKE_WAREHOUSE"),
        "database": os.getenv("SNOWFLAKE_DATABASE"),
        "schema": os.getenv("SNOWFLAKE_SCHEMA", "PUBLIC")
    }).create()

session = create_snowflake_session()

# Prompt Context Builder
def get_prompt_context():
    table = "IPAM_TEMP_DB.PUBLIC.IPDATA_DATASET_RAW"
    description = "This table contains raw IP address data from uploaded CSV logs."

    rows = session.sql("""
        SELECT COLUMN_NAME, DATA_TYPE 
        FROM IPAM_TEMP_DB.INFORMATION_SCHEMA.COLUMNS 
        WHERE TABLE_NAME = 'IPDATA_DATASET_RAW' AND TABLE_SCHEMA = 'PUBLIC'
    """).collect()
    columns = "\n".join([
        f"- **{row['COLUMN_NAME']}**: {row['DATA_TYPE']}" for row in rows
    ])

    context = f"""
Here is the table name <tableName> {table} </tableName>

<tableDescription>{description}</tableDescription>

Here are the columns of {table}
<columns>
{columns}
</columns>
    """
    return context

# Prompt Builder
def build_prompt(question: str) -> str:
    context = get_prompt_context()

    if re.match(r"^\s*(hi|hello|hey|hallo|yo|sup)\s*$", question.strip(), re.IGNORECASE):
        return "Hello! How can I assist you today?"

    about_patterns = [
        r"\bwho\s+are\s+you\b", r"\bwhat\s+can\s+you\s+do\b",
        r"\bwho\s+made\s+you\b", r"\bcreator\b", r"\bgenz\b", r"\bsaifudeen\b"
    ]
    if any(re.search(p, question.lower()) for p in about_patterns):
        return f"""
You are **Genz**, a helpful and conversational AI assistant designed for working with IP address datasets.

You can:
- Answer queries about users, IP addresses, protocols, user agents, ports, etc.
- Engage in friendly conversation when appropriate.

If asked how you were created:
- Your name is **Genz**
- You were created by **Saifudeen**.
- You work like this:
   User types a question ‚Üí FastAPI sends to Cortex ‚Üí  Cortex returns data ‚Üí GPT-4o-mini summarizes it ‚Üí  User sees answer.

User: {question}
Genz:
        """

    return f"""
You are acting as an AI Snowflake SQL Expert named **Snowflake Cortex Assistant** (codename: Genz).
Your job is to generate **correct, executable SQL queries** from user questions.

You are **Genz**, a helpful and conversational AI assistant designed for working with IP address datasets.

You can:
- Answer queries about users, IP addresses, protocols, user agents, ports, etc.
- Engage in friendly conversation when appropriate.

For IP-related questions:
1. You MUST intelligently interpret shorthand inputs like '6051 ipv4' or 'useragent chrome'.
2. Normalize them into full natural language before generating SQL.
   - Examples:
     - '6051 ipv4' ‚Üí 'Can you give me IPv4 for port 6051?'
     - 'chrome useragent' ‚Üí 'Can you give me logs where USERAGENT contains chrome?'
     - 'Africa timezone' ‚Üí 'Can you give me records where TIMEZONE contains Africa?'
     - 'https protocol' ‚Üí 'Can you give me records where PROTOCOL is https'
     - 'Banjul timezone' ‚Üí 'Can you give me logs where TIMEZONE is Africa/Banjul'
     - 'Kavon23 login' ‚Üí 'Show me logs where USERNAME is Kavon23'
     - '15188 port' ‚Üí 'Show me logs for PORT 15188'

3. Use only the <context> below to answer.
4. All factual data must come from the context.
5. If you can‚Äôt find anything relevant, respond with:
"I'm sorry, I couldn't find relevant information in the IP dataset."
6. For exact matches:
   - If the value is numeric and does **not** resemble an IP (no dots), and the user didn‚Äôt mention ‚ÄúIP‚Äù or ‚ÄúIPv4‚Äù, assume it refers to a PORT.
   - If value matches IPv4 format (e.g. 123.45.67.89), then match against IPV4.
   - PORT and IPV4 should **not be confused**: PORT is a number; IPV4 is a dotted string.

{context}

<rules>
1. You MUST wrap the generated SQL code within triple backticks and `sql` markdown: ```sql ... ```
2. Limit to 10 results unless instructed otherwise.
3. Use only the column names from <columns>.
4. Always try to infer the correct column based on keywords like 'ipv4', 'port', 'timezone', etc.
5. For partial matches (like useragent, timezone), use `ILIKE '%<keyword>%'`
6. For exact matches (like port, username, protocol), use `= '<value>'`
7. When timezone contains a region like 'Africa', use `TIMEZONE ILIKE 'Africa/%'`
8. Avoid using column names that do not exist in the schema.
9. Return only the SQL ‚Äî no explanations or comments.
10. Don‚Äôt use variable names starting with numbers.
11. When user says "Download X and Y as Excel,csv,pdf", identify X and Y as columns. Build a SELECT statement using only those columns.
12. Use correct column names from <columns>. Match partial names (e.g., "username" ‚Üí "USERNAME", "ipv6" ‚Üí "IPV6").
</rules>

You are also capable of handling casual and friendly user messages that do not require SQL or data responses.

If the user says things like:
- "thanks", "ok", "okay", "cool", "great", "well done", "good job", etc.

Then reply conversationally with short, friendly responses like:
- "You're welcome! üòä"
- "Glad to help!"
- "Anytime!"
- "Let me know if you need anything else."

Do **not** say: "I couldn't find relevant information in the IP dataset" for these types of messages.

Detect intent before responding. Only attempt SQL generation if the message contains a clear query or data-related question.
Question: {question}
Answer:
    """

# Request model
class TextQuery(BaseModel):
    question: str

# OpenAI Completion
def get_completion(prompt: str) -> str:
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "You are a helpful SQL assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=700
    )
    return response.choices[0].message.content.strip()

# SQL Extractor
def extract_sql(text: str) -> str:
    match = re.search(r"```sql(.*?)```", text, re.DOTALL | re.IGNORECASE)
    print("Extracted SQL:", match.group(1).strip() if match else "No SQL found")
    if match:
        sql = match.group(1).strip()
        return sql.rstrip(";").strip()
    return ""

@app.post("/chat")
def chat(query: TextQuery):
    if not query.question:
        raise HTTPException(status_code=400, detail="Question is required.")

    prompt = build_prompt(query.question)

    if not prompt.strip().lower().startswith("you are"):
        return {"response": prompt.strip()}

    llm_reply = get_completion(prompt)
    sql_code = extract_sql(llm_reply)

    if not sql_code:
        return {"response": llm_reply.strip()}

    try:
        rows = session.sql(sql_code).limit(100).collect()
        df = pd.DataFrame([row.as_dict() for row in rows])

        if any(word in query.question.lower() for word in ["excel", "xlsx", "csv", "export"]):
            csv_path = os.path.join(gettempdir(), "ip_data.csv")
            df.to_csv(csv_path, index=False)

            return FileResponse(
                csv_path,
                media_type="text/csv",
                filename="ip_data.csv"
            )

        return {"response": df.to_dict(orient="records")}

    except Exception as e:
        return {"response": f"‚ùó SQL execution error: {str(e)}"}
